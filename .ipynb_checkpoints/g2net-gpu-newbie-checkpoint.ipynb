{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018353,
     "end_time": "2021-09-16T07:22:30.857975",
     "exception": false,
     "start_time": "2021-09-16T07:22:30.839622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "- Basically a 1D CNN starter with bandpass. Filter size hard-coded from [https://www.kaggle.com/kit716/grav-wave-detection](https://www.kaggle.com/kit716/grav-wave-detection) which uses the simple architecture from https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103 \n",
    "- Added inference to @hidehisaarai1213 's PyTorch starter, iteration order changed from Y.Nakama's pipeline: \"iter on loader first then load model\" to \"load model first then iter the loader\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017142,
     "end_time": "2021-09-16T07:22:30.892621",
     "exception": false,
     "start_time": "2021-09-16T07:22:30.875479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:30.938971Z",
     "iopub.status.busy": "2021-09-16T07:22:30.938392Z",
     "iopub.status.idle": "2021-09-16T07:22:37.704913Z",
     "shell.execute_reply": "2021-09-16T07:22:37.703844Z",
     "shell.execute_reply.started": "2021-09-16T04:44:15.710811Z"
    },
    "papermill": {
     "duration": 6.795398,
     "end_time": "2021-09-16T07:22:37.705094",
     "exception": false,
     "start_time": "2021-09-16T07:22:30.909696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-62b79ef950d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[1;31m# for reading TFRecord Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfds\u001b[0m  \u001b[1;31m# for making tf.data.Dataset to return numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import tensorflow as tf  # for reading TFRecord Dataset\n",
    "import tensorflow_datasets as tfds  # for making tf.data.Dataset to return numpy arrays\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:37.785259Z",
     "iopub.status.busy": "2021-09-16T07:22:37.784504Z",
     "iopub.status.idle": "2021-09-16T07:22:37.788024Z",
     "shell.execute_reply": "2021-09-16T07:22:37.787579Z",
     "shell.execute_reply.started": "2021-09-16T04:44:22.3944Z"
    },
    "papermill": {
     "duration": 0.065524,
     "end_time": "2021-09-16T07:22:37.788148",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.722624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEDIR = Path(\"./\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017002,
     "end_time": "2021-09-16T07:22:37.822593",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.805591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:37.863883Z",
     "iopub.status.busy": "2021-09-16T07:22:37.863083Z",
     "iopub.status.idle": "2021-09-16T07:22:37.865967Z",
     "shell.execute_reply": "2021-09-16T07:22:37.865389Z",
     "shell.execute_reply.started": "2021-09-16T04:44:22.443943Z"
    },
    "papermill": {
     "duration": 0.026158,
     "end_time": "2021-09-16T07:22:37.866082",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.839924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    print_freq = 1000\n",
    "    num_workers = 4\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    model_name = \"1dcnn\"\n",
    "    epochs = 5\n",
    "    T_max = 3\n",
    "    lr = 1e-4\n",
    "    min_lr = 1e-7\n",
    "    batch_size = 64\n",
    "    val_batch_size = 100\n",
    "    weight_decay = 1e-5\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    target_size = 1\n",
    "    target_col = \"target\"\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3]  # [0, 1, 2, 3, 4]\n",
    "    train = True\n",
    "    bandpass_params = dict(lf=20, \n",
    "                           hf=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016932,
     "end_time": "2021-09-16T07:22:37.900161",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.883229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:37.942294Z",
     "iopub.status.busy": "2021-09-16T07:22:37.941714Z",
     "iopub.status.idle": "2021-09-16T07:22:37.947930Z",
     "shell.execute_reply": "2021-09-16T07:22:37.947484Z",
     "shell.execute_reply.started": "2021-09-16T04:44:22.45428Z"
    },
    "papermill": {
     "duration": 0.030656,
     "end_time": "2021-09-16T07:22:37.948046",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.917390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=SAVEDIR / 'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016899,
     "end_time": "2021-09-16T07:22:37.982400",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.965501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TFRecord Loader\n",
    "\n",
    "This is the heart of this notebook. Instead of using PyTorch's Dataset and DataLoader, here I define custom Loader that reads samples from TFRecords.\n",
    "\n",
    "FYI, there's a library that does the same thing, but its implementation is not optimized, so it's slower.\n",
    "\n",
    "https://github.com/vahidk/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:38.030204Z",
     "iopub.status.busy": "2021-09-16T07:22:38.029548Z",
     "iopub.status.idle": "2021-09-16T07:22:39.838249Z",
     "shell.execute_reply": "2021-09-16T07:22:39.837507Z",
     "shell.execute_reply.started": "2021-09-16T04:44:22.468713Z"
    },
    "papermill": {
     "duration": 1.838454,
     "end_time": "2021-09-16T07:22:39.838379",
     "exception": false,
     "start_time": "2021-09-16T07:22:37.999925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kds-fe725ba5a3259c712812aed413cfe61fc3827b135988e28e694e515c\n",
      "gs://kds-ecd1c4515f350d79d5a8d2e9a07df52885278c6cce43ad116c8b03cb\n",
      "gs://kds-2c0b3b314d8607851f6dddea1e976aab52547b68de5117cf3e76a85a\n",
      "gs://kds-965e4184655c915b40b37bf7e5d3c8e310dd5751e8a2e1f5bdc8d70d\n",
      "train_files:  20\n"
     ]
    }
   ],
   "source": [
    "gcs_paths = []\n",
    "for i, j in [(0, 4), (5, 9), (10, 14), (15, 19)]:\n",
    "    path = f\"g2net-waveform-tfrecords-train-{i}-{j}\"\n",
    "    n_trial = 0\n",
    "    while True:\n",
    "        try:\n",
    "            gcs_path = KaggleDatasets().get_gcs_path(path)\n",
    "            gcs_paths.append(gcs_path)\n",
    "            print(gcs_path)\n",
    "            break\n",
    "        except:\n",
    "            if n_trial > 10:\n",
    "                break\n",
    "            n_trial += 1\n",
    "            continue\n",
    "            \n",
    "all_files = []\n",
    "for path in gcs_paths:\n",
    "    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/train*.tfrecords\"))))\n",
    "    \n",
    "print(\"train_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:39.880315Z",
     "iopub.status.busy": "2021-09-16T07:22:39.879564Z",
     "iopub.status.idle": "2021-09-16T07:22:39.882177Z",
     "shell.execute_reply": "2021-09-16T07:22:39.881755Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.143735Z"
    },
    "papermill": {
     "duration": 0.025069,
     "end_time": "2021-09-16T07:22:39.882284",
     "exception": false,
     "start_time": "2021-09-16T07:22:39.857215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_data_items(fileids, train=True):\n",
    "    \"\"\"\n",
    "    Count the number of samples.\n",
    "    Each of the TFRecord datasets is designed to contain 28000 samples for train\n",
    "    22500 for test.\n",
    "    \"\"\"\n",
    "    sizes = 28000 if train else 22500\n",
    "    return len(fileids) * sizes\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018402,
     "end_time": "2021-09-16T07:22:39.918907",
     "exception": false,
     "start_time": "2021-09-16T07:22:39.900505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bandpass\n",
    "\n",
    "Modified from various notebooks and https://www.kaggle.com/c/g2net-gravitational-wave-detection/discussion/261721#1458564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:39.962438Z",
     "iopub.status.busy": "2021-09-16T07:22:39.961747Z",
     "iopub.status.idle": "2021-09-16T07:22:39.964503Z",
     "shell.execute_reply": "2021-09-16T07:22:39.964080Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.152539Z"
    },
    "papermill": {
     "duration": 0.027365,
     "end_time": "2021-09-16T07:22:39.964600",
     "exception": false,
     "start_time": "2021-09-16T07:22:39.937235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bandpass(x, lf=20, hf=500, order=8, sr=2048):\n",
    "    '''\n",
    "    Cell 33 of https://www.gw-openscience.org/LVT151012data/LOSC_Event_tutorial_LVT151012.html\n",
    "    https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "    '''\n",
    "    sos = signal.butter(order, [lf, hf], btype=\"bandpass\", output=\"sos\", fs=sr)\n",
    "    normalization = np.sqrt((hf - lf) / (sr / 2))\n",
    "    if x.ndim ==2:\n",
    "        for i in range(3):\n",
    "            x[i] = signal.sosfilt(sos, x[i]) * normalization\n",
    "    elif x.ndim == 3: # batch\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(3):\n",
    "                x[i, j] = signal.sosfilt(sos, x[i, j]) * normalization\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.014335Z",
     "iopub.status.busy": "2021-09-16T07:22:40.013585Z",
     "iopub.status.idle": "2021-09-16T07:22:40.016039Z",
     "shell.execute_reply": "2021-09-16T07:22:40.016484Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.164651Z"
    },
    "papermill": {
     "duration": 0.033625,
     "end_time": "2021-09-16T07:22:40.016607",
     "exception": false,
     "start_time": "2021-09-16T07:22:39.982982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_wave(wave):\n",
    "    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "    normalized_waves = []\n",
    "    scaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n",
    "    for i in range(3):\n",
    "#         normalized_wave = wave[i] / tf.math.reduce_max(wave[i])\n",
    "        normalized_wave = wave[i] / scaling[i]\n",
    "        normalized_waves.append(normalized_wave)\n",
    "    wave = tf.stack(normalized_waves, axis=0)\n",
    "    wave = tf.cast(wave, tf.float32)\n",
    "    return wave\n",
    "\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1]), example[\"wave_id\"]\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_id):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), example[\"wave_id\"] if return_image_id else 0\n",
    "\n",
    "\n",
    "def get_dataset(files, batch_size=16, repeat=False, cache=False, \n",
    "                shuffle=False, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    if cache:\n",
    "        # You'll need around 15GB RAM if you'd like to cache val dataset, and 50~60GB RAM for train dataset.\n",
    "        ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 2)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    if labeled:\n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.061852Z",
     "iopub.status.busy": "2021-09-16T07:22:40.061118Z",
     "iopub.status.idle": "2021-09-16T07:22:40.063652Z",
     "shell.execute_reply": "2021-09-16T07:22:40.063222Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.182246Z"
    },
    "papermill": {
     "duration": 0.028583,
     "end_time": "2021-09-16T07:22:40.063758",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.035175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFRecordDataLoader:\n",
    "    def __init__(self, files, batch_size=32, cache=False, train=True, \n",
    "                              repeat=False, shuffle=False, labeled=True, \n",
    "                              return_image_ids=True):\n",
    "        self.ds = get_dataset(\n",
    "            files, \n",
    "            batch_size=batch_size,\n",
    "            cache=cache,\n",
    "            repeat=repeat,\n",
    "            shuffle=shuffle,\n",
    "            labeled=labeled,\n",
    "            return_image_ids=return_image_ids)\n",
    "        \n",
    "        self.num_examples = count_data_items(files, labeled)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.labeled = labeled\n",
    "        self.return_image_ids = return_image_ids\n",
    "        self._iterator = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self._iterator is None:\n",
    "            self._iterator = iter(self.ds)\n",
    "        else:\n",
    "            self._reset()\n",
    "        return self._iterator\n",
    "\n",
    "    def _reset(self):\n",
    "        self._iterator = iter(self.ds)\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = next(self._iterator)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_batches = self.num_examples // self.batch_size\n",
    "        if self.num_examples % self.batch_size == 0:\n",
    "            return n_batches\n",
    "        else:\n",
    "            return n_batches + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019378,
     "end_time": "2021-09-16T07:22:40.101368",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.081990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.151117Z",
     "iopub.status.busy": "2021-09-16T07:22:40.150395Z",
     "iopub.status.idle": "2021-09-16T07:22:40.153156Z",
     "shell.execute_reply": "2021-09-16T07:22:40.152758Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.198387Z"
    },
    "papermill": {
     "duration": 0.03319,
     "end_time": "2021-09-16T07:22:40.153256",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.120066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, kernel_size=64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=32),\n",
    "            nn.AvgPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=32),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=16),\n",
    "            nn.AvgPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=16),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 11, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, x, pos=None):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018195,
     "end_time": "2021-09-16T07:22:40.190048",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.171853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.235031Z",
     "iopub.status.busy": "2021-09-16T07:22:40.234298Z",
     "iopub.status.idle": "2021-09-16T07:22:40.236494Z",
     "shell.execute_reply": "2021-09-16T07:22:40.236928Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.216415Z"
    },
    "papermill": {
     "duration": 0.028121,
     "end_time": "2021-09-16T07:22:40.237082",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.208961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def max_memory_allocated():\n",
    "    MB = 1024.0 * 1024.0\n",
    "    mem = torch.cuda.max_memory_allocated() / MB\n",
    "    return f\"{mem:.0f} MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018124,
     "end_time": "2021-09-16T07:22:40.273483",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.255359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.329703Z",
     "iopub.status.busy": "2021-09-16T07:22:40.328972Z",
     "iopub.status.idle": "2021-09-16T07:22:40.331610Z",
     "shell.execute_reply": "2021-09-16T07:22:40.331224Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.230407Z"
    },
    "papermill": {
     "duration": 0.039977,
     "end_time": "2021-09-16T07:22:40.331735",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.291758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(files, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    train_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size, \n",
    "        shuffle=True)\n",
    "    for step, d in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(x)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('Epoch: [{0}/{1}][{2}/{3}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  'Elapsed: {remain:s} '\n",
    "                  'Max mem: {mem:s}'\n",
    "                  .format(\n",
    "                   epoch+1, CFG.epochs, step, len(train_loader),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_last_lr()[0],\n",
    "                   remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                   mem=max_memory_allocated()))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(files, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    filenames = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    valid_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size * 2, shuffle=False)\n",
    "    for step, d in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets.extend(d[1].reshape(-1).tolist())\n",
    "        filenames.extend([f.decode(\"UTF-8\") for f in d[2]])\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    predictions = np.concatenate(preds).reshape(-1)\n",
    "    return losses.avg, predictions, np.array(targets), np.array(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018071,
     "end_time": "2021-09-16T07:22:40.367977",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.349906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.418297Z",
     "iopub.status.busy": "2021-09-16T07:22:40.417580Z",
     "iopub.status.idle": "2021-09-16T07:22:40.419877Z",
     "shell.execute_reply": "2021-09-16T07:22:40.420287Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.25458Z"
    },
    "papermill": {
     "duration": 0.034223,
     "end_time": "2021-09-16T07:22:40.420400",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.386177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(train_tfrecords: np.ndarray, val_tfrecords: np.ndarray, fold: int):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                             mode='min', \n",
    "                                                             factor=CFG.factor, \n",
    "                                                             patience=CFG.patience, \n",
    "                                                             verbose=True, \n",
    "                                                             eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                             T_max=CFG.T_max, \n",
    "                                                             eta_min=CFG.min_lr, \n",
    "                                                             last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                       T_0=CFG.T_0, \n",
    "                                                                       T_mult=1, \n",
    "                                                                       eta_min=CFG.min_lr, \n",
    "                                                                       last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CNN1d()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(\"\\n\\n\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_tfrecords, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds, targets, files = valid_fn(val_tfrecords, model, criterion, device)\n",
    "        valid_result_df = pd.DataFrame({\"target\": targets, \"preds\": preds, \"id\": files})\n",
    "        \n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(targets, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "    \n",
    "    valid_result_df[\"preds\"] = torch.load(SAVEDIR / f\"{CFG.model_name}_fold{fold}_best_loss.pth\",\n",
    "                                          map_location=\"cpu\")[\"preds\"]\n",
    "\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T07:22:40.467376Z",
     "iopub.status.busy": "2021-09-16T07:22:40.466651Z",
     "iopub.status.idle": "2021-09-16T15:06:59.972589Z",
     "shell.execute_reply": "2021-09-16T15:06:59.972068Z",
     "shell.execute_reply.started": "2021-09-16T04:44:25.275133Z"
    },
    "papermill": {
     "duration": 27859.533922,
     "end_time": "2021-09-16T15:06:59.972746",
     "exception": false,
     "start_time": "2021-09-16T07:22:40.438824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [1/5][0/7000] Loss: 0.7253(0.7253) Grad: 15.4019  LR: 0.000100  Elapsed: 0m 4s (remain 491m 55s) Max mem: 340 MB\n",
      "Epoch: [1/5][1000/7000] Loss: 0.5553(0.6204) Grad: 2.0464  LR: 0.000100  Elapsed: 2m 58s (remain 17m 52s) Max mem: 359 MB\n",
      "Epoch: [1/5][2000/7000] Loss: 0.6050(0.5658) Grad: 2.2560  LR: 0.000100  Elapsed: 5m 52s (remain 14m 40s) Max mem: 359 MB\n",
      "Epoch: [1/5][3000/7000] Loss: 0.4321(0.5385) Grad: 1.8809  LR: 0.000100  Elapsed: 8m 46s (remain 11m 41s) Max mem: 359 MB\n",
      "Epoch: [1/5][4000/7000] Loss: 0.4675(0.5215) Grad: 1.3852  LR: 0.000100  Elapsed: 11m 39s (remain 8m 44s) Max mem: 359 MB\n",
      "Epoch: [1/5][5000/7000] Loss: 0.3990(0.5107) Grad: 1.4228  LR: 0.000100  Elapsed: 14m 32s (remain 5m 48s) Max mem: 359 MB\n",
      "Epoch: [1/5][6000/7000] Loss: 0.5149(0.5029) Grad: 1.3005  LR: 0.000100  Elapsed: 17m 23s (remain 2m 53s) Max mem: 359 MB\n",
      "EVAL: [0/875] Data 0.681 (0.681) Elapsed 0m 0s (remain 12m 29s) Loss: 0.4207(0.4207) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4972  avg_val_loss: 0.4443  time: 1408s\n",
      "Epoch 1 - Score: 0.8560\n",
      "Epoch 1 - Save Best Score: 0.8560 Model\n",
      "Epoch 1 - Save Best Loss: 0.4443 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/5][0/7000] Loss: 0.4543(0.4543) Grad: 1.5554  LR: 0.000075  Elapsed: 0m 2s (remain 333m 1s) Max mem: 359 MB\n",
      "Epoch: [2/5][1000/7000] Loss: 0.5241(0.4537) Grad: 1.7128  LR: 0.000075  Elapsed: 2m 57s (remain 17m 45s) Max mem: 359 MB\n",
      "Epoch: [2/5][2000/7000] Loss: 0.3947(0.4513) Grad: 1.1962  LR: 0.000075  Elapsed: 5m 51s (remain 14m 37s) Max mem: 359 MB\n",
      "Epoch: [2/5][3000/7000] Loss: 0.4975(0.4513) Grad: 1.4686  LR: 0.000075  Elapsed: 8m 46s (remain 11m 42s) Max mem: 359 MB\n",
      "Epoch: [2/5][4000/7000] Loss: 0.5628(0.4497) Grad: 1.8454  LR: 0.000075  Elapsed: 11m 40s (remain 8m 45s) Max mem: 359 MB\n",
      "Epoch: [2/5][5000/7000] Loss: 0.4274(0.4488) Grad: 1.0432  LR: 0.000075  Elapsed: 14m 34s (remain 5m 49s) Max mem: 359 MB\n",
      "Epoch: [2/5][6000/7000] Loss: 0.4218(0.4480) Grad: 1.6685  LR: 0.000075  Elapsed: 17m 29s (remain 2m 54s) Max mem: 359 MB\n",
      "EVAL: [0/875] Data 0.832 (0.832) Elapsed 0m 1s (remain 14m 35s) Loss: 0.4213(0.4213) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4475  avg_val_loss: 0.4611  time: 1396s\n",
      "Epoch 2 - Score: 0.8616\n",
      "Epoch 2 - Save Best Score: 0.8616 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/5][0/7000] Loss: 0.4320(0.4320) Grad: 1.5394  LR: 0.000025  Elapsed: 0m 3s (remain 395m 45s) Max mem: 359 MB\n",
      "Epoch: [3/5][1000/7000] Loss: 0.4152(0.4360) Grad: 0.9497  LR: 0.000025  Elapsed: 2m 56s (remain 17m 35s) Max mem: 359 MB\n",
      "Epoch: [3/5][2000/7000] Loss: 0.3587(0.4344) Grad: 0.9767  LR: 0.000025  Elapsed: 5m 48s (remain 14m 30s) Max mem: 359 MB\n",
      "Epoch: [3/5][3000/7000] Loss: 0.3748(0.4347) Grad: 1.1711  LR: 0.000025  Elapsed: 8m 40s (remain 11m 33s) Max mem: 359 MB\n",
      "Epoch: [3/5][4000/7000] Loss: 0.4182(0.4336) Grad: 1.2432  LR: 0.000025  Elapsed: 11m 32s (remain 8m 39s) Max mem: 359 MB\n",
      "Epoch: [3/5][5000/7000] Loss: 0.4420(0.4330) Grad: 1.2301  LR: 0.000025  Elapsed: 14m 23s (remain 5m 45s) Max mem: 359 MB\n",
      "Epoch: [3/5][6000/7000] Loss: 0.4562(0.4324) Grad: 1.2925  LR: 0.000025  Elapsed: 17m 15s (remain 2m 52s) Max mem: 359 MB\n",
      "EVAL: [0/875] Data 0.758 (0.758) Elapsed 0m 0s (remain 14m 9s) Loss: 0.4078(0.4078) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4320  avg_val_loss: 0.4275  time: 1423s\n",
      "Epoch 3 - Score: 0.8664\n",
      "Epoch 3 - Save Best Score: 0.8664 Model\n",
      "Epoch 3 - Save Best Loss: 0.4275 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/5][0/7000] Loss: 0.4236(0.4236) Grad: 1.9376  LR: 0.000000  Elapsed: 0m 3s (remain 413m 53s) Max mem: 359 MB\n",
      "Epoch: [4/5][1000/7000] Loss: 0.4060(0.4293) Grad: 1.2841  LR: 0.000000  Elapsed: 2m 57s (remain 17m 46s) Max mem: 359 MB\n",
      "Epoch: [4/5][2000/7000] Loss: 0.3737(0.4281) Grad: 1.7857  LR: 0.000000  Elapsed: 5m 51s (remain 14m 38s) Max mem: 359 MB\n",
      "Epoch: [4/5][3000/7000] Loss: 0.4562(0.4286) Grad: 2.4528  LR: 0.000000  Elapsed: 8m 44s (remain 11m 38s) Max mem: 359 MB\n",
      "Epoch: [4/5][4000/7000] Loss: 0.3833(0.4272) Grad: 1.6523  LR: 0.000000  Elapsed: 11m 38s (remain 8m 43s) Max mem: 359 MB\n",
      "Epoch: [4/5][5000/7000] Loss: 0.5401(0.4268) Grad: 1.4155  LR: 0.000000  Elapsed: 14m 32s (remain 5m 48s) Max mem: 359 MB\n",
      "Epoch: [4/5][6000/7000] Loss: 0.4837(0.4259) Grad: 1.5287  LR: 0.000000  Elapsed: 17m 25s (remain 2m 54s) Max mem: 359 MB\n",
      "EVAL: [0/875] Data 0.596 (0.596) Elapsed 0m 0s (remain 14m 4s) Loss: 0.4070(0.4070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.4252  avg_val_loss: 0.4323  time: 1384s\n",
      "Epoch 4 - Score: 0.8669\n",
      "Epoch 4 - Save Best Score: 0.8669 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [5/5][0/7000] Loss: 0.3375(0.3375) Grad: 1.2674  LR: 0.000025  Elapsed: 0m 2s (remain 338m 20s) Max mem: 359 MB\n",
      "Epoch: [5/5][1000/7000] Loss: 0.5645(0.4293) Grad: 1.9623  LR: 0.000025  Elapsed: 2m 57s (remain 17m 43s) Max mem: 359 MB\n",
      "Epoch: [5/5][2000/7000] Loss: 0.4230(0.4278) Grad: 1.3457  LR: 0.000025  Elapsed: 5m 50s (remain 14m 34s) Max mem: 359 MB\n",
      "Epoch: [5/5][3000/7000] Loss: 0.4553(0.4282) Grad: 1.3204  LR: 0.000025  Elapsed: 8m 43s (remain 11m 37s) Max mem: 359 MB\n",
      "Epoch: [5/5][4000/7000] Loss: 0.4547(0.4275) Grad: 1.6034  LR: 0.000025  Elapsed: 11m 37s (remain 8m 43s) Max mem: 359 MB\n",
      "Epoch: [5/5][5000/7000] Loss: 0.4216(0.4273) Grad: 1.5153  LR: 0.000025  Elapsed: 14m 29s (remain 5m 47s) Max mem: 359 MB\n",
      "Epoch: [5/5][6000/7000] Loss: 0.4590(0.4268) Grad: 1.8610  LR: 0.000025  Elapsed: 17m 22s (remain 2m 53s) Max mem: 359 MB\n",
      "EVAL: [0/875] Data 0.700 (0.700) Elapsed 0m 0s (remain 13m 40s) Loss: 0.4023(0.4023) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.4264  avg_val_loss: 0.4377  time: 1397s\n",
      "Epoch 5 - Score: 0.8668\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.8664\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [1/5][0/7000] Loss: 0.7431(0.7431) Grad: 13.4667  LR: 0.000100  Elapsed: 0m 3s (remain 351m 33s) Max mem: 359 MB\n",
      "Epoch: [1/5][1000/7000] Loss: 0.5670(0.6283) Grad: 2.3826  LR: 0.000100  Elapsed: 2m 55s (remain 17m 34s) Max mem: 360 MB\n",
      "Epoch: [1/5][2000/7000] Loss: 0.5016(0.5730) Grad: 1.5860  LR: 0.000100  Elapsed: 5m 49s (remain 14m 33s) Max mem: 360 MB\n",
      "Epoch: [1/5][3000/7000] Loss: 0.3739(0.5441) Grad: 1.2224  LR: 0.000100  Elapsed: 8m 41s (remain 11m 35s) Max mem: 360 MB\n",
      "Epoch: [1/5][4000/7000] Loss: 0.5030(0.5262) Grad: 1.2110  LR: 0.000100  Elapsed: 11m 35s (remain 8m 40s) Max mem: 360 MB\n",
      "Epoch: [1/5][5000/7000] Loss: 0.4874(0.5148) Grad: 1.3197  LR: 0.000100  Elapsed: 14m 26s (remain 5m 46s) Max mem: 360 MB\n",
      "Epoch: [1/5][6000/7000] Loss: 0.4174(0.5060) Grad: 1.3126  LR: 0.000100  Elapsed: 18m 28s (remain 3m 4s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.725 (0.725) Elapsed 0m 0s (remain 12m 57s) Loss: 0.4191(0.4191) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4999  avg_val_loss: 0.4596  time: 1449s\n",
      "Epoch 1 - Score: 0.8520\n",
      "Epoch 1 - Save Best Score: 0.8520 Model\n",
      "Epoch 1 - Save Best Loss: 0.4596 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/5][0/7000] Loss: 0.5434(0.5434) Grad: 0.7845  LR: 0.000075  Elapsed: 0m 3s (remain 382m 9s) Max mem: 360 MB\n",
      "Epoch: [2/5][1000/7000] Loss: 0.4162(0.4539) Grad: 1.2662  LR: 0.000075  Elapsed: 2m 55s (remain 17m 32s) Max mem: 360 MB\n",
      "Epoch: [2/5][2000/7000] Loss: 0.5670(0.4522) Grad: 1.9730  LR: 0.000075  Elapsed: 5m 47s (remain 14m 28s) Max mem: 360 MB\n",
      "Epoch: [2/5][3000/7000] Loss: 0.4630(0.4511) Grad: 0.8496  LR: 0.000075  Elapsed: 8m 38s (remain 11m 31s) Max mem: 360 MB\n",
      "Epoch: [2/5][4000/7000] Loss: 0.5224(0.4492) Grad: 2.0929  LR: 0.000075  Elapsed: 11m 31s (remain 8m 38s) Max mem: 360 MB\n",
      "Epoch: [2/5][5000/7000] Loss: 0.4826(0.4484) Grad: 1.4892  LR: 0.000075  Elapsed: 14m 26s (remain 5m 46s) Max mem: 360 MB\n",
      "Epoch: [2/5][6000/7000] Loss: 0.4363(0.4471) Grad: 1.3361  LR: 0.000075  Elapsed: 17m 20s (remain 2m 53s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.653 (0.653) Elapsed 0m 0s (remain 12m 49s) Loss: 0.4184(0.4184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4467  avg_val_loss: 0.4438  time: 1392s\n",
      "Epoch 2 - Score: 0.8589\n",
      "Epoch 2 - Save Best Score: 0.8589 Model\n",
      "Epoch 2 - Save Best Loss: 0.4438 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/5][0/7000] Loss: 0.4104(0.4104) Grad: 1.3862  LR: 0.000025  Elapsed: 0m 2s (remain 346m 35s) Max mem: 360 MB\n",
      "Epoch: [3/5][1000/7000] Loss: 0.3712(0.4359) Grad: 0.8286  LR: 0.000025  Elapsed: 2m 55s (remain 17m 34s) Max mem: 360 MB\n",
      "Epoch: [3/5][2000/7000] Loss: 0.3789(0.4345) Grad: 1.4561  LR: 0.000025  Elapsed: 5m 47s (remain 14m 28s) Max mem: 360 MB\n",
      "Epoch: [3/5][3000/7000] Loss: 0.3859(0.4339) Grad: 1.8068  LR: 0.000025  Elapsed: 8m 39s (remain 11m 32s) Max mem: 360 MB\n",
      "Epoch: [3/5][4000/7000] Loss: 0.3781(0.4325) Grad: 1.4939  LR: 0.000025  Elapsed: 11m 32s (remain 8m 38s) Max mem: 360 MB\n",
      "Epoch: [3/5][5000/7000] Loss: 0.3684(0.4320) Grad: 1.3804  LR: 0.000025  Elapsed: 14m 23s (remain 5m 45s) Max mem: 360 MB\n",
      "Epoch: [3/5][6000/7000] Loss: 0.3960(0.4313) Grad: 1.5108  LR: 0.000025  Elapsed: 17m 16s (remain 2m 52s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.760 (0.760) Elapsed 0m 0s (remain 13m 20s) Loss: 0.4148(0.4148) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4311  avg_val_loss: 0.4431  time: 1375s\n",
      "Epoch 3 - Score: 0.8646\n",
      "Epoch 3 - Save Best Score: 0.8646 Model\n",
      "Epoch 3 - Save Best Loss: 0.4431 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/5][0/7000] Loss: 0.3707(0.3707) Grad: 1.5067  LR: 0.000000  Elapsed: 0m 2s (remain 341m 2s) Max mem: 360 MB\n",
      "Epoch: [4/5][1000/7000] Loss: 0.4208(0.4302) Grad: 1.6099  LR: 0.000000  Elapsed: 2m 55s (remain 17m 30s) Max mem: 360 MB\n",
      "Epoch: [4/5][2000/7000] Loss: 0.5475(0.4300) Grad: 1.7094  LR: 0.000000  Elapsed: 5m 47s (remain 14m 27s) Max mem: 360 MB\n",
      "Epoch: [4/5][3000/7000] Loss: 0.4349(0.4295) Grad: 0.9543  LR: 0.000000  Elapsed: 8m 39s (remain 11m 32s) Max mem: 360 MB\n",
      "Epoch: [4/5][4000/7000] Loss: 0.3896(0.4279) Grad: 1.4615  LR: 0.000000  Elapsed: 11m 31s (remain 8m 38s) Max mem: 360 MB\n",
      "Epoch: [4/5][5000/7000] Loss: 0.3497(0.4275) Grad: 1.2440  LR: 0.000000  Elapsed: 14m 24s (remain 5m 45s) Max mem: 360 MB\n",
      "Epoch: [4/5][6000/7000] Loss: 0.4670(0.4267) Grad: 1.9161  LR: 0.000000  Elapsed: 17m 16s (remain 2m 52s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.731 (0.731) Elapsed 0m 0s (remain 12m 54s) Loss: 0.4041(0.4041) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.4260  avg_val_loss: 0.4378  time: 1375s\n",
      "Epoch 4 - Score: 0.8656\n",
      "Epoch 4 - Save Best Score: 0.8656 Model\n",
      "Epoch 4 - Save Best Loss: 0.4378 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [5/5][0/7000] Loss: 0.3691(0.3691) Grad: 1.3548  LR: 0.000025  Elapsed: 0m 2s (remain 311m 56s) Max mem: 360 MB\n",
      "Epoch: [5/5][1000/7000] Loss: 0.4505(0.4296) Grad: 2.0098  LR: 0.000025  Elapsed: 2m 55s (remain 17m 31s) Max mem: 360 MB\n",
      "Epoch: [5/5][2000/7000] Loss: 0.5441(0.4281) Grad: 2.3880  LR: 0.000025  Elapsed: 5m 47s (remain 14m 28s) Max mem: 360 MB\n",
      "Epoch: [5/5][3000/7000] Loss: 0.3798(0.4282) Grad: 1.0132  LR: 0.000025  Elapsed: 8m 40s (remain 11m 33s) Max mem: 360 MB\n",
      "Epoch: [5/5][4000/7000] Loss: 0.4023(0.4268) Grad: 2.0093  LR: 0.000025  Elapsed: 11m 33s (remain 8m 39s) Max mem: 360 MB\n",
      "Epoch: [5/5][5000/7000] Loss: 0.3880(0.4268) Grad: 1.6268  LR: 0.000025  Elapsed: 14m 26s (remain 5m 46s) Max mem: 360 MB\n",
      "Epoch: [5/5][6000/7000] Loss: 0.4196(0.4264) Grad: 1.3298  LR: 0.000025  Elapsed: 17m 19s (remain 2m 53s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.703 (0.703) Elapsed 0m 0s (remain 12m 57s) Loss: 0.4103(0.4103) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.4263  avg_val_loss: 0.4414  time: 1376s\n",
      "Epoch 5 - Score: 0.8657\n",
      "Epoch 5 - Save Best Score: 0.8657 Model\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.8656\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [1/5][0/7000] Loss: 0.7015(0.7015) Grad: 15.4602  LR: 0.000100  Elapsed: 0m 2s (remain 332m 42s) Max mem: 360 MB\n",
      "Epoch: [1/5][1000/7000] Loss: 0.5045(0.6115) Grad: 2.1054  LR: 0.000100  Elapsed: 2m 57s (remain 17m 44s) Max mem: 360 MB\n",
      "Epoch: [1/5][2000/7000] Loss: 0.5485(0.5632) Grad: 2.0410  LR: 0.000100  Elapsed: 5m 50s (remain 14m 36s) Max mem: 360 MB\n",
      "Epoch: [1/5][3000/7000] Loss: 0.4819(0.5381) Grad: 1.5312  LR: 0.000100  Elapsed: 8m 43s (remain 11m 37s) Max mem: 360 MB\n",
      "Epoch: [1/5][4000/7000] Loss: 0.4969(0.5227) Grad: 1.1932  LR: 0.000100  Elapsed: 11m 35s (remain 8m 41s) Max mem: 360 MB\n",
      "Epoch: [1/5][5000/7000] Loss: 0.4762(0.5112) Grad: 0.9735  LR: 0.000100  Elapsed: 14m 27s (remain 5m 46s) Max mem: 360 MB\n",
      "Epoch: [1/5][6000/7000] Loss: 0.5579(0.5029) Grad: 1.9994  LR: 0.000100  Elapsed: 17m 19s (remain 2m 52s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.726 (0.726) Elapsed 0m 0s (remain 13m 3s) Loss: 0.5306(0.5306) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4967  avg_val_loss: 0.4557  time: 1373s\n",
      "Epoch 1 - Score: 0.8513\n",
      "Epoch 1 - Save Best Score: 0.8513 Model\n",
      "Epoch 1 - Save Best Loss: 0.4557 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/5][0/7000] Loss: 0.3084(0.3084) Grad: 1.0503  LR: 0.000075  Elapsed: 0m 2s (remain 342m 31s) Max mem: 360 MB\n",
      "Epoch: [2/5][1000/7000] Loss: 0.4772(0.4531) Grad: 1.7629  LR: 0.000075  Elapsed: 2m 55s (remain 17m 30s) Max mem: 360 MB\n",
      "Epoch: [2/5][2000/7000] Loss: 0.4740(0.4517) Grad: 1.2288  LR: 0.000075  Elapsed: 5m 47s (remain 14m 29s) Max mem: 360 MB\n",
      "Epoch: [2/5][3000/7000] Loss: 0.3681(0.4503) Grad: 0.9950  LR: 0.000075  Elapsed: 8m 38s (remain 11m 31s) Max mem: 360 MB\n",
      "Epoch: [2/5][4000/7000] Loss: 0.4025(0.4499) Grad: 1.1412  LR: 0.000075  Elapsed: 11m 30s (remain 8m 37s) Max mem: 360 MB\n",
      "Epoch: [2/5][5000/7000] Loss: 0.5280(0.4484) Grad: 2.1331  LR: 0.000075  Elapsed: 14m 22s (remain 5m 44s) Max mem: 360 MB\n",
      "Epoch: [2/5][6000/7000] Loss: 0.4139(0.4470) Grad: 1.2276  LR: 0.000075  Elapsed: 17m 14s (remain 2m 52s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.842 (0.842) Elapsed 0m 1s (remain 14m 51s) Loss: 0.4978(0.4978) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4462  avg_val_loss: 0.4640  time: 1370s\n",
      "Epoch 2 - Score: 0.8584\n",
      "Epoch 2 - Save Best Score: 0.8584 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/5][0/7000] Loss: 0.4216(0.4216) Grad: 1.2105  LR: 0.000025  Elapsed: 0m 2s (remain 342m 22s) Max mem: 360 MB\n",
      "Epoch: [3/5][1000/7000] Loss: 0.4833(0.4330) Grad: 1.6076  LR: 0.000025  Elapsed: 2m 54s (remain 17m 27s) Max mem: 360 MB\n",
      "Epoch: [3/5][2000/7000] Loss: 0.5100(0.4331) Grad: 2.8132  LR: 0.000025  Elapsed: 5m 45s (remain 14m 23s) Max mem: 360 MB\n",
      "Epoch: [3/5][3000/7000] Loss: 0.3955(0.4329) Grad: 1.2543  LR: 0.000025  Elapsed: 8m 35s (remain 11m 27s) Max mem: 360 MB\n",
      "Epoch: [3/5][4000/7000] Loss: 0.3290(0.4334) Grad: 1.2161  LR: 0.000025  Elapsed: 11m 26s (remain 8m 34s) Max mem: 360 MB\n",
      "Epoch: [3/5][5000/7000] Loss: 0.4336(0.4324) Grad: 1.6552  LR: 0.000025  Elapsed: 14m 17s (remain 5m 42s) Max mem: 360 MB\n",
      "Epoch: [3/5][6000/7000] Loss: 0.4013(0.4315) Grad: 1.8793  LR: 0.000025  Elapsed: 17m 17s (remain 2m 52s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.906 (0.906) Elapsed 0m 1s (remain 15m 27s) Loss: 0.4863(0.4863) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4309  avg_val_loss: 0.4374  time: 1380s\n",
      "Epoch 3 - Score: 0.8628\n",
      "Epoch 3 - Save Best Score: 0.8628 Model\n",
      "Epoch 3 - Save Best Loss: 0.4374 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/5][0/7000] Loss: 0.4569(0.4569) Grad: 2.0066  LR: 0.000000  Elapsed: 0m 3s (remain 352m 24s) Max mem: 360 MB\n",
      "Epoch: [4/5][1000/7000] Loss: 0.4055(0.4292) Grad: 1.0587  LR: 0.000000  Elapsed: 2m 54s (remain 17m 24s) Max mem: 360 MB\n",
      "Epoch: [4/5][2000/7000] Loss: 0.3596(0.4287) Grad: 1.8127  LR: 0.000000  Elapsed: 5m 45s (remain 14m 22s) Max mem: 360 MB\n",
      "Epoch: [4/5][3000/7000] Loss: 0.4656(0.4291) Grad: 1.5342  LR: 0.000000  Elapsed: 8m 36s (remain 11m 28s) Max mem: 360 MB\n",
      "Epoch: [4/5][4000/7000] Loss: 0.3970(0.4292) Grad: 1.4709  LR: 0.000000  Elapsed: 11m 26s (remain 8m 34s) Max mem: 360 MB\n",
      "Epoch: [4/5][5000/7000] Loss: 0.4135(0.4280) Grad: 1.7386  LR: 0.000000  Elapsed: 14m 16s (remain 5m 42s) Max mem: 360 MB\n",
      "Epoch: [4/5][6000/7000] Loss: 0.2618(0.4267) Grad: 1.1714  LR: 0.000000  Elapsed: 17m 6s (remain 2m 50s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.686 (0.686) Elapsed 0m 0s (remain 12m 14s) Loss: 0.4983(0.4983) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.4257  avg_val_loss: 0.4459  time: 1373s\n",
      "Epoch 4 - Score: 0.8634\n",
      "Epoch 4 - Save Best Score: 0.8634 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [5/5][0/7000] Loss: 0.4303(0.4303) Grad: 1.4695  LR: 0.000025  Elapsed: 0m 2s (remain 336m 43s) Max mem: 360 MB\n",
      "Epoch: [5/5][1000/7000] Loss: 0.6238(0.4274) Grad: 2.7437  LR: 0.000025  Elapsed: 2m 54s (remain 17m 26s) Max mem: 360 MB\n",
      "Epoch: [5/5][2000/7000] Loss: 0.4637(0.4277) Grad: 1.4663  LR: 0.000025  Elapsed: 5m 46s (remain 14m 24s) Max mem: 360 MB\n",
      "Epoch: [5/5][3000/7000] Loss: 0.5380(0.4275) Grad: 2.2573  LR: 0.000025  Elapsed: 8m 37s (remain 11m 29s) Max mem: 360 MB\n",
      "Epoch: [5/5][4000/7000] Loss: 0.5000(0.4281) Grad: 1.6621  LR: 0.000025  Elapsed: 11m 28s (remain 8m 36s) Max mem: 360 MB\n",
      "Epoch: [5/5][5000/7000] Loss: 0.5602(0.4273) Grad: 2.6144  LR: 0.000025  Elapsed: 14m 19s (remain 5m 43s) Max mem: 360 MB\n",
      "Epoch: [5/5][6000/7000] Loss: 0.4163(0.4263) Grad: 1.5759  LR: 0.000025  Elapsed: 17m 10s (remain 2m 51s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.702 (0.702) Elapsed 0m 0s (remain 12m 27s) Loss: 0.4901(0.4901) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.4257  avg_val_loss: 0.4377  time: 1370s\n",
      "Epoch 5 - Score: 0.8633\n",
      "========== fold: 2 result ==========\n",
      "Score: 0.8628\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [1/5][0/7000] Loss: 0.7124(0.7124) Grad: 12.0316  LR: 0.000100  Elapsed: 0m 2s (remain 338m 7s) Max mem: 360 MB\n",
      "Epoch: [1/5][1000/7000] Loss: 0.5828(0.5979) Grad: 2.5982  LR: 0.000100  Elapsed: 2m 54s (remain 17m 25s) Max mem: 360 MB\n",
      "Epoch: [1/5][2000/7000] Loss: 0.3974(0.5532) Grad: 1.3003  LR: 0.000100  Elapsed: 5m 46s (remain 14m 25s) Max mem: 360 MB\n",
      "Epoch: [1/5][3000/7000] Loss: 0.5314(0.5292) Grad: 1.4170  LR: 0.000100  Elapsed: 8m 36s (remain 11m 28s) Max mem: 360 MB\n",
      "Epoch: [1/5][4000/7000] Loss: 0.4502(0.5152) Grad: 1.3957  LR: 0.000100  Elapsed: 11m 28s (remain 8m 35s) Max mem: 360 MB\n",
      "Epoch: [1/5][5000/7000] Loss: 0.4080(0.5058) Grad: 0.9684  LR: 0.000100  Elapsed: 14m 18s (remain 5m 43s) Max mem: 360 MB\n",
      "Epoch: [1/5][6000/7000] Loss: 0.4133(0.4981) Grad: 1.3636  LR: 0.000100  Elapsed: 17m 9s (remain 2m 51s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.894 (0.894) Elapsed 0m 1s (remain 15m 16s) Loss: 0.4784(0.4784) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4932  avg_val_loss: 0.4685  time: 1379s\n",
      "Epoch 1 - Score: 0.8555\n",
      "Epoch 1 - Save Best Score: 0.8555 Model\n",
      "Epoch 1 - Save Best Loss: 0.4685 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/5][0/7000] Loss: 0.4856(0.4856) Grad: 1.6071  LR: 0.000075  Elapsed: 0m 3s (remain 372m 57s) Max mem: 360 MB\n",
      "Epoch: [2/5][1000/7000] Loss: 0.4699(0.4532) Grad: 1.2263  LR: 0.000075  Elapsed: 2m 55s (remain 17m 29s) Max mem: 360 MB\n",
      "Epoch: [2/5][2000/7000] Loss: 0.5427(0.4521) Grad: 1.9811  LR: 0.000075  Elapsed: 5m 46s (remain 14m 24s) Max mem: 360 MB\n",
      "Epoch: [2/5][3000/7000] Loss: 0.4607(0.4507) Grad: 1.4607  LR: 0.000075  Elapsed: 8m 35s (remain 11m 27s) Max mem: 360 MB\n",
      "Epoch: [2/5][4000/7000] Loss: 0.4444(0.4496) Grad: 1.4444  LR: 0.000075  Elapsed: 11m 26s (remain 8m 34s) Max mem: 360 MB\n",
      "Epoch: [2/5][5000/7000] Loss: 0.4533(0.4487) Grad: 0.9315  LR: 0.000075  Elapsed: 14m 18s (remain 5m 43s) Max mem: 360 MB\n",
      "Epoch: [2/5][6000/7000] Loss: 0.5151(0.4473) Grad: 1.0166  LR: 0.000075  Elapsed: 17m 9s (remain 2m 51s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.764 (0.764) Elapsed 0m 1s (remain 14m 57s) Loss: 0.4442(0.4442) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4470  avg_val_loss: 0.4403  time: 1397s\n",
      "Epoch 2 - Score: 0.8609\n",
      "Epoch 2 - Save Best Score: 0.8609 Model\n",
      "Epoch 2 - Save Best Loss: 0.4403 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/5][0/7000] Loss: 0.5015(0.5015) Grad: 2.5163  LR: 0.000025  Elapsed: 0m 3s (remain 359m 23s) Max mem: 360 MB\n",
      "Epoch: [3/5][1000/7000] Loss: 0.4084(0.4352) Grad: 1.6139  LR: 0.000025  Elapsed: 2m 59s (remain 17m 55s) Max mem: 360 MB\n",
      "Epoch: [3/5][2000/7000] Loss: 0.4614(0.4342) Grad: 1.6815  LR: 0.000025  Elapsed: 5m 54s (remain 14m 45s) Max mem: 360 MB\n",
      "Epoch: [3/5][3000/7000] Loss: 0.4740(0.4338) Grad: 1.4107  LR: 0.000025  Elapsed: 8m 49s (remain 11m 45s) Max mem: 360 MB\n",
      "Epoch: [3/5][4000/7000] Loss: 0.5007(0.4333) Grad: 1.7480  LR: 0.000025  Elapsed: 11m 45s (remain 8m 48s) Max mem: 360 MB\n",
      "Epoch: [3/5][5000/7000] Loss: 0.4307(0.4328) Grad: 1.4282  LR: 0.000025  Elapsed: 14m 40s (remain 5m 51s) Max mem: 360 MB\n",
      "Epoch: [3/5][6000/7000] Loss: 0.4405(0.4316) Grad: 1.6904  LR: 0.000025  Elapsed: 17m 34s (remain 2m 55s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.663 (0.663) Elapsed 0m 0s (remain 12m 25s) Loss: 0.4389(0.4389) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4315  avg_val_loss: 0.4352  time: 1412s\n",
      "Epoch 3 - Score: 0.8657\n",
      "Epoch 3 - Save Best Score: 0.8657 Model\n",
      "Epoch 3 - Save Best Loss: 0.4352 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/5][0/7000] Loss: 0.4954(0.4954) Grad: 2.7415  LR: 0.000000  Elapsed: 0m 2s (remain 338m 15s) Max mem: 360 MB\n",
      "Epoch: [4/5][1000/7000] Loss: 0.4396(0.4290) Grad: 1.6756  LR: 0.000000  Elapsed: 2m 56s (remain 17m 40s) Max mem: 360 MB\n",
      "Epoch: [4/5][2000/7000] Loss: 0.4510(0.4287) Grad: 1.3065  LR: 0.000000  Elapsed: 5m 51s (remain 14m 39s) Max mem: 360 MB\n",
      "Epoch: [4/5][3000/7000] Loss: 0.4706(0.4281) Grad: 2.8171  LR: 0.000000  Elapsed: 8m 46s (remain 11m 41s) Max mem: 360 MB\n",
      "Epoch: [4/5][4000/7000] Loss: 0.4061(0.4278) Grad: 2.5705  LR: 0.000000  Elapsed: 11m 41s (remain 8m 45s) Max mem: 360 MB\n",
      "Epoch: [4/5][5000/7000] Loss: 0.4839(0.4274) Grad: 1.7287  LR: 0.000000  Elapsed: 14m 35s (remain 5m 49s) Max mem: 360 MB\n",
      "Epoch: [4/5][6000/7000] Loss: 0.4142(0.4262) Grad: 1.6384  LR: 0.000000  Elapsed: 17m 30s (remain 2m 54s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.685 (0.685) Elapsed 0m 0s (remain 14m 6s) Loss: 0.4415(0.4415) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.4257  avg_val_loss: 0.4388  time: 1409s\n",
      "Epoch 4 - Score: 0.8661\n",
      "Epoch 4 - Save Best Score: 0.8661 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [5/5][0/7000] Loss: 0.4041(0.4041) Grad: 1.9349  LR: 0.000025  Elapsed: 0m 3s (remain 427m 25s) Max mem: 360 MB\n",
      "Epoch: [5/5][1000/7000] Loss: 0.4642(0.4284) Grad: 1.8024  LR: 0.000025  Elapsed: 2m 58s (remain 17m 51s) Max mem: 360 MB\n",
      "Epoch: [5/5][2000/7000] Loss: 0.4045(0.4285) Grad: 1.2149  LR: 0.000025  Elapsed: 5m 52s (remain 14m 41s) Max mem: 360 MB\n",
      "Epoch: [5/5][3000/7000] Loss: 0.5134(0.4282) Grad: 2.0348  LR: 0.000025  Elapsed: 8m 48s (remain 11m 44s) Max mem: 360 MB\n",
      "Epoch: [5/5][4000/7000] Loss: 0.4078(0.4278) Grad: 1.2354  LR: 0.000025  Elapsed: 11m 43s (remain 8m 47s) Max mem: 360 MB\n",
      "Epoch: [5/5][5000/7000] Loss: 0.5160(0.4275) Grad: 2.5174  LR: 0.000025  Elapsed: 14m 37s (remain 5m 50s) Max mem: 360 MB\n",
      "Epoch: [5/5][6000/7000] Loss: 0.3974(0.4266) Grad: 1.6143  LR: 0.000025  Elapsed: 17m 32s (remain 2m 55s) Max mem: 360 MB\n",
      "EVAL: [0/875] Data 0.722 (0.722) Elapsed 0m 0s (remain 12m 59s) Loss: 0.4270(0.4270) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.4268  avg_val_loss: 0.4312  time: 1412s\n",
      "Epoch 5 - Score: 0.8664\n",
      "Epoch 5 - Save Best Score: 0.8664 Model\n",
      "Epoch 5 - Save Best Loss: 0.4312 Model\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.8664\n",
      "========== CV ==========\n",
      "Score: 0.8651\n"
     ]
    }
   ],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df['preds'].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}')\n",
    "\n",
    "if CFG.train:\n",
    "    # train \n",
    "    oof_df = pd.DataFrame()\n",
    "    kf = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "    folds = list(kf.split(all_files))\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            trn_idx, val_idx = folds[fold]\n",
    "            train_files = all_files[trn_idx]\n",
    "            valid_files = all_files[val_idx]\n",
    "            _oof_df = train_loop(train_files, valid_files, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "    # CV result\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    # save result\n",
    "    oof_df.to_csv(SAVEDIR / 'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084632,
     "end_time": "2021-09-16T15:07:00.145781",
     "exception": false,
     "start_time": "2021-09-16T15:07:00.061149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T15:07:00.319001Z",
     "iopub.status.busy": "2021-09-16T15:07:00.318428Z",
     "iopub.status.idle": "2021-09-16T15:07:00.377730Z",
     "shell.execute_reply": "2021-09-16T15:07:00.377179Z"
    },
    "papermill": {
     "duration": 0.14803,
     "end_time": "2021-09-16T15:07:00.377851",
     "exception": false,
     "start_time": "2021-09-16T15:07:00.229821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for fold  in CFG.trn_fold:\n",
    "    states.append(torch.load(os.path.join(SAVEDIR, f'{CFG.model_name}_fold{fold}_best_score.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T15:07:00.553971Z",
     "iopub.status.busy": "2021-09-16T15:07:00.553198Z",
     "iopub.status.idle": "2021-09-16T15:07:01.319328Z",
     "shell.execute_reply": "2021-09-16T15:07:01.318843Z"
    },
    "papermill": {
     "duration": 0.857434,
     "end_time": "2021-09-16T15:07:01.319449",
     "exception": false,
     "start_time": "2021-09-16T15:07:00.462015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kds-3af0f2c792ef9e6f5d3c87bb80c99a263fc9707106cd595aaf02e3eb\n",
      "gs://kds-9eb3135b732342c0aec8339381fec9fd19f8a7ad94ca9c31ad51fe2b\n",
      "test_files:  10\n"
     ]
    }
   ],
   "source": [
    "gcs_paths = []\n",
    "for i, j in [(0, 4), (5, 9)]:\n",
    "    path = f\"g2net-waveform-tfrecords-test-{i}-{j}\"\n",
    "    n_trial = 0\n",
    "    while True:\n",
    "        try:\n",
    "            gcs_path = KaggleDatasets().get_gcs_path(path)\n",
    "            gcs_paths.append(gcs_path)\n",
    "            print(gcs_path)\n",
    "            break\n",
    "        except:\n",
    "            if n_trial > 10:\n",
    "                break\n",
    "            n_trial += 1\n",
    "            continue\n",
    "            \n",
    "all_files = []\n",
    "for path in gcs_paths:\n",
    "    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/test*.tfrecords\"))))\n",
    "    \n",
    "print(\"test_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T15:07:01.498935Z",
     "iopub.status.busy": "2021-09-16T15:07:01.498179Z",
     "iopub.status.idle": "2021-09-16T15:33:26.045675Z",
     "shell.execute_reply": "2021-09-16T15:33:26.046103Z"
    },
    "papermill": {
     "duration": 1584.641098,
     "end_time": "2021-09-16T15:33:26.046284",
     "exception": false,
     "start_time": "2021-09-16T15:07:01.405186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2260it [06:43,  5.60it/s]\n",
      "  0%|          | 0/2250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2260it [06:40,  5.64it/s]\n",
      "  0%|          | 0/2250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2260it [06:28,  5.82it/s]\n",
      "  0%|          | 0/2250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2260it [06:31,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "model= CNN1d()\n",
    "model.to(device)\n",
    "\n",
    "wave_ids = []\n",
    "probs_all = []\n",
    "\n",
    "for fold, state in enumerate(states):\n",
    "    tqdm.write(f\"\\n\\nFold{fold}\")\n",
    "    \n",
    "    model.load_state_dict(state['model'])\n",
    "    model.eval()\n",
    "    probs = []\n",
    "\n",
    "    test_loader = TFRecordDataLoader(all_files, batch_size=CFG.val_batch_size, \n",
    "                                     shuffle=False, labeled=False)\n",
    "\n",
    "    for i, d in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "        preds = y_preds.sigmoid().to('cpu').numpy()\n",
    "        probs.append(preds)\n",
    "\n",
    "        if fold==0: # same test loader, no need to do this the second time\n",
    "            wave_ids.append(d[1].astype('U13'))\n",
    "\n",
    "    probs = np.concatenate(probs)\n",
    "    probs_all.append(probs)\n",
    "\n",
    "probs_avg = np.asarray(probs_all).mean(axis=0).flatten()\n",
    "wave_ids = np.concatenate(wave_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T15:33:30.218915Z",
     "iopub.status.busy": "2021-09-16T15:33:30.218074Z",
     "iopub.status.idle": "2021-09-16T15:33:30.856575Z",
     "shell.execute_reply": "2021-09-16T15:33:30.856079Z"
    },
    "papermill": {
     "duration": 2.72646,
     "end_time": "2021-09-16T15:33:30.856730",
     "exception": false,
     "start_time": "2021-09-16T15:33:28.130270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'id': wave_ids, 'target': probs_avg})\n",
    "# Save test dataframe to disk\n",
    "folds = '_'.join([str(s) for s in CFG.trn_fold])\n",
    "test_df.to_csv(f'{CFG.model_name}_folds_{folds}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29470.610436,
   "end_time": "2021-09-16T15:33:35.243771",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-16T07:22:24.633335",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
